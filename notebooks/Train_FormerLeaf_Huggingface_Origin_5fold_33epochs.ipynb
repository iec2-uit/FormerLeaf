{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "papermill": {
      "duration": 8637.721674,
      "end_time": "2020-11-23T15:56:40.428882",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-11-23T13:32:42.707208",
      "version": "2.1.0"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OAf2Zn7frOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f62c26a8-c509-4562-b65f-b58832f7349f"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jul  8 04:00:28 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCRoQMin1oGs",
        "outputId": "cf0e2d24-5070-49c8-870f-af425de0fcfd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hJT3EK2mIAi",
        "outputId": "4fe2e086-29bc-47c7-d344-58d027aa35a5"
      },
      "source": [
        "%cd drive/MyDrive/Colab\\ Notebooks\n",
        "!pwd\n",
        "%cd ViT_Leaf_Desease/\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n",
            "/content/drive/MyDrive/Colab Notebooks\n",
            "/content/drive/.shortcut-targets-by-id/1-4N4zpXcGEEVM2Xa98yuO9H9JVYtL060/ViT_Leaf_Desease\n",
            "/content/drive/.shortcut-targets-by-id/1-4N4zpXcGEEVM2Xa98yuO9H9JVYtL060/ViT_Leaf_Desease\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWkc1BUGdmcw",
        "outputId": "87ef7c2a-0a7e-4598-b9d2-6a2fa6706d6f"
      },
      "source": [
        "package_paths = [\n",
        "    './input/pytorch-image-models/pytorch-image-models-master', #'../input/efficientnet-pytorch-07/efficientnet_pytorch-0.7.0'\n",
        "    './input/FMix'\n",
        "]\n",
        "import sys; \n",
        "\n",
        "for pth in package_paths:\n",
        "    sys.path.append(pth)\n",
        "print(sys.path)    \n",
        "#from fmix import sample_mask, make_low_freq_image, binarise_mask"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython', './input/pytorch-image-models/pytorch-image-models-master', './input/FMix']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBNIAxAD50Sx",
        "outputId": "825a6d73-68bb-4786-9179-a0ad4062103c"
      },
      "source": [
        "!pip3 install timm\n",
        "!pip3 install pydicom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting timm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/08/1ccaf8d516935666b7fa5f6aaddf157c66208ea0c93bb847ae09f166354f/timm-0.4.9-py3-none-any.whl (346kB)\n",
            "\r\u001b[K     |█                               | 10kB 23.7MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 29.4MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30kB 21.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 40kB 17.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 51kB 14.9MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 61kB 16.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 71kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 81kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 92kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 102kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 112kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 122kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 133kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 143kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 153kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 163kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 174kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 184kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 194kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 204kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 215kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 225kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 235kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 245kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 256kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 266kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 276kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 286kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 296kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 307kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 317kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 327kB 13.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 337kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 348kB 13.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.10.0+cu102)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.9.0+cu102)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.9\n",
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/15/df16546bc59bfca390cf072d473fb2c8acd4231636f64356593a63137e55/pydicom-2.1.2-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 13.3MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9YIO3H_dmc2"
      },
      "source": [
        "from glob import glob\n",
        "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
        "import cv2\n",
        "from skimage import io\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "import cv2\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.nn.modules.loss import _WeightedLoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import timm\n",
        "\n",
        "import sklearn\n",
        "import warnings\n",
        "import joblib\n",
        "from sklearn.metrics import roc_auc_score, log_loss\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "import cv2\n",
        "import pydicom\n",
        "#from efficientnet_pytorch import EfficientNet\n",
        "from scipy.ndimage.interpolation import zoom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJgdr7ycdmc3"
      },
      "source": [
        "CFG = {\n",
        "    'fold_num': 5,\n",
        "    'seed': 719,\n",
        "    #'model_arch': 'tf_efficientnet_b4_ns',\n",
        "    'model_arch': 'vit_base_patch16_224',\n",
        "    #'model_arch': 'vit_small_resnet50d_s3_224',\n",
        "    #'img_size': 512,\n",
        "    'img_size': 224,\n",
        "    'epochs': 33,\n",
        "    'train_bs': 32,\n",
        "    'valid_bs': 32,\n",
        "    'T_0': 10,\n",
        "    'lr': 1e-4,\n",
        "    'min_lr': 1e-6,\n",
        "    'weight_decay':1e-6,\n",
        "    'num_workers': 4,\n",
        "    'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
        "    'verbose_step': 1,\n",
        "    'device': 'cuda:0'\n",
        "  #  'device': 'cpu'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "VZowuKAsdmc4",
        "outputId": "c75598fb-ee9c-40ce-cb8e-223a9cb039cb"
      },
      "source": [
        "train = pd.read_csv('./input/cassava-leaf-disease-classification/train.csv')\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000015157.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000201771.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100042118.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000723321.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000812911.jpg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         image_id  label\n",
              "0  1000015157.jpg      0\n",
              "1  1000201771.jpg      3\n",
              "2   100042118.jpg      1\n",
              "3  1000723321.jpg      1\n",
              "4  1000812911.jpg      3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J0eXl9Idmc5",
        "outputId": "0a6cbaf0-5402-472a-9f66-e8e9060dee6a"
      },
      "source": [
        "train.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    13158\n",
              "4     2577\n",
              "2     2386\n",
              "1     2189\n",
              "0     1087\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFZ6nnucdmc5"
      },
      "source": [
        "> We could do stratified validation split in each fold to make each fold's train and validation set looks like the whole train set in target distributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "Tv1pWfSbdmc6",
        "outputId": "c40cbe52-37bf-463c-a9df-cd112740c114"
      },
      "source": [
        "submission = pd.read_csv('./input/cassava-leaf-disease-classification/sample_submission.csv')\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2216849948.jpg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         image_id  label\n",
              "0  2216849948.jpg      4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C27ElN51dmc6"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUID6b6ndmc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "05a49703-07ee-4ad1-9222-2e0a4295e37b"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "def get_img(path):\n",
        "    im_bgr = cv2.imread(path)\n",
        "    im_rgb = im_bgr[:, :, ::-1]\n",
        "    #print(im_rgb)\n",
        "    return im_rgb\n",
        "\n",
        "img = get_img('./input/cassava-leaf-disease-classification/train_images/1000015157.jpg')\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5e831e32fbba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mim_rgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./input/cassava-leaf-disease-classification/train_images/1000015157.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5e831e32fbba>\u001b[0m in \u001b[0;36mget_img\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mim_bgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mim_rgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_bgr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#print(im_rgb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBhBzURVdmc7"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55GsDyVIdmc7"
      },
      "source": [
        "def rand_bbox(size, lam):\n",
        "    W = size[0]\n",
        "    H = size[1]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = np.int(W * cut_rat)\n",
        "    cut_h = np.int(H * cut_rat)\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "\n",
        "class CassavaDataset(Dataset):\n",
        "    def __init__(self, df, data_root, \n",
        "                 transforms=None, \n",
        "                 output_label=True, \n",
        "                 one_hot_label=False,\n",
        "                 do_fmix=False, \n",
        "                 fmix_params={\n",
        "                     'alpha': 1., \n",
        "                     'decay_power': 3., \n",
        "                     'shape': (CFG['img_size'], CFG['img_size']),\n",
        "                     'max_soft': True, \n",
        "                     'reformulate': False\n",
        "                 },\n",
        "                 do_cutmix=False,\n",
        "                 cutmix_params={\n",
        "                     'alpha': 1,\n",
        "                 }\n",
        "                ):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.df = df.reset_index(drop=True).copy()\n",
        "        self.transforms = transforms\n",
        "        self.data_root = data_root\n",
        "        self.do_fmix = do_fmix\n",
        "        self.fmix_params = fmix_params\n",
        "        self.do_cutmix = do_cutmix\n",
        "        self.cutmix_params = cutmix_params\n",
        "        \n",
        "        self.output_label = output_label\n",
        "        self.one_hot_label = one_hot_label\n",
        "        \n",
        "        if output_label == True:\n",
        "            self.labels = self.df['label'].values\n",
        "            #print(self.labels)\n",
        "            \n",
        "            if one_hot_label is True:\n",
        "                self.labels = np.eye(self.df['label'].max()+1)[self.labels]\n",
        "                #print(self.labels)\n",
        "            \n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index: int):\n",
        "        \n",
        "        # get labels\n",
        "        if self.output_label:\n",
        "            target = self.labels[index]\n",
        "          \n",
        "        img  = get_img(\"{}/{}\".format(self.data_root, self.df.loc[index]['image_id']))\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(image=img)['image']\n",
        "        \n",
        "        if self.do_fmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n",
        "            with torch.no_grad():\n",
        "                #lam, mask = sample_mask(**self.fmix_params)\n",
        "                \n",
        "                lam = np.clip(np.random.beta(self.fmix_params['alpha'], self.fmix_params['alpha']),0.6,0.7)\n",
        "                \n",
        "                # Make mask, get mean / std\n",
        "                mask = make_low_freq_image(self.fmix_params['decay_power'], self.fmix_params['shape'])\n",
        "                mask = binarise_mask(mask, lam, self.fmix_params['shape'], self.fmix_params['max_soft'])\n",
        "    \n",
        "                fmix_ix = np.random.choice(self.df.index, size=1)[0]\n",
        "                fmix_img  = get_img(\"{}/{}\".format(self.data_root, self.df.iloc[fmix_ix]['image_id']))\n",
        "\n",
        "                if self.transforms:\n",
        "                    fmix_img = self.transforms(image=fmix_img)['image']\n",
        "\n",
        "                mask_torch = torch.from_numpy(mask)\n",
        "                \n",
        "                # mix image\n",
        "                img = mask_torch*img+(1.-mask_torch)*fmix_img\n",
        "\n",
        "                #print(mask.shape)\n",
        "\n",
        "                #assert self.output_label==True and self.one_hot_label==True\n",
        "\n",
        "                # mix target\n",
        "                rate = mask.sum()/CFG['img_size']/CFG['img_size']\n",
        "                target = rate*target + (1.-rate)*self.labels[fmix_ix]\n",
        "                #print(target, mask, img)\n",
        "                #assert False\n",
        "        \n",
        "        if self.do_cutmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n",
        "            #print(img.sum(), img.shape)\n",
        "            with torch.no_grad():\n",
        "                cmix_ix = np.random.choice(self.df.index, size=1)[0]\n",
        "                cmix_img  = get_img(\"{}/{}\".format(self.data_root, self.df.iloc[cmix_ix]['image_id']))\n",
        "                if self.transforms:\n",
        "                    cmix_img = self.transforms(image=cmix_img)['image']\n",
        "                    \n",
        "                lam = np.clip(np.random.beta(self.cutmix_params['alpha'], self.cutmix_params['alpha']),0.3,0.4)\n",
        "                bbx1, bby1, bbx2, bby2 = rand_bbox((CFG['img_size'], CFG['img_size']), lam)\n",
        "\n",
        "                img[:, bbx1:bbx2, bby1:bby2] = cmix_img[:, bbx1:bbx2, bby1:bby2]\n",
        "\n",
        "                rate = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (CFG['img_size'] * CFG['img_size']))\n",
        "                target = rate*target + (1.-rate)*self.labels[cmix_ix]\n",
        "                \n",
        "            #print('-', img.sum())\n",
        "            #print(target)\n",
        "            #assert False\n",
        "                            \n",
        "        # do label smoothing\n",
        "        #print(type(img), type(target))\n",
        "        if self.output_label == True:\n",
        "            return img, target\n",
        "        else:\n",
        "            return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "545Xu8qG8NAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "837678ad-b3ec-4f39-a91b-c17b17749b4b"
      },
      "source": [
        "#.!pip3 install --upgrade albumentations\n",
        "!pip3 install git+https://github.com/albumentations-team/albumentations.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/albumentations-team/albumentations.git\n",
            "  Cloning https://github.com/albumentations-team/albumentations.git to /tmp/pip-req-build-y1hwyl7k\n",
            "  Running command git clone -q https://github.com/albumentations-team/albumentations.git /tmp/pip-req-build-y1hwyl7k\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (0.16.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.0.0) (4.1.2.30)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2.5.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (7.1.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (1.1.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations==1.0.0) (4.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.0.0) (1.15.0)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-1.0.0-cp37-none-any.whl size=98151 sha256=c05b8253ce8825271290badeccad44c8a06afd3848a98bbda10f88d47112e565\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a2yvck66/wheels/e2/85/3e/2a40fac5cc1f43ced656603bb2fca1327b30ec7de1b1b66517\n",
            "Successfully built albumentations\n",
            "Installing collected packages: albumentations\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT1kO4dCdmc8"
      },
      "source": [
        "# Define Train\\Validation Image Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBuv20Ipdmc8"
      },
      "source": [
        "from albumentations import (\n",
        "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
        "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
        "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
        "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
        ")\n",
        "\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "def get_train_transforms():\n",
        "    return Compose([\n",
        "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
        "            Transpose(p=0.5),\n",
        "            HorizontalFlip(p=0.5),\n",
        "            VerticalFlip(p=0.5),\n",
        "            ShiftScaleRotate(p=0.5),\n",
        "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
        "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
        "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
        "            CoarseDropout(p=0.5),\n",
        "            Cutout(p=0.5),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ], p=1.)\n",
        "  \n",
        "        \n",
        "def get_valid_transforms():\n",
        "    return Compose([\n",
        "            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n",
        "            Resize(CFG['img_size'], CFG['img_size']),\n",
        "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ], p=1.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82DHgqG90c7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fdec4ce-33da-4576-94a2-bf34dd64c223"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/d5/c6c23ad75491467a9a84e526ef2364e523d45e2b0fae28a7cbe8689e7e84/transformers-4.8.1-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 13.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: huggingface-hub, tokenizers, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWgR50Pvdmc9"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKpqhWZYdmc9"
      },
      "source": [
        "from transformers import ViTModel, ViTConfig, ViTForImageClassification\n",
        "\n",
        "class CassvaImgClassifier(nn.Module):\n",
        "    def __init__(self, model_arch, n_class, pretrained=False):\n",
        "        super().__init__()\n",
        "        self.model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "        #efficientNet initilization\n",
        "       # n_features = self.model.classifier.in_features\n",
        "        #self.model.classifier = nn.Linear(n_features, n_class)\n",
        "\n",
        "        #ViT initilization\n",
        "        self.model.classifier = nn.Linear(self.model.classifier.in_features, n_class)\n",
        "       \n",
        "       \n",
        "        '''\n",
        "        self.model.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            #nn.Linear(n_features, hidden_size,bias=True), nn.ELU(),\n",
        "            nn.Linear(n_features, n_class, bias=True)\n",
        "        )\n",
        "        '''\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a41XljavTfpN"
      },
      "source": [
        "def f1_loss(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:\n",
        "    '''Calculate F1 score. Can work with gpu tensors\n",
        "    \n",
        "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        `ndim` == 1. 0 <= val <= 1\n",
        "    \n",
        "    Reference\n",
        "    ---------\n",
        "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
        "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
        "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
        "    \n",
        "    '''\n",
        "    assert y_true.ndim == 1\n",
        "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
        "    \n",
        "    if y_pred.ndim == 2:\n",
        "        y_pred = y_pred.argmax(dim=1)\n",
        "    \n",
        "    print(y_true)\n",
        "    print(y_pred)\n",
        "    tp = (y_true * y_pred).sum().astype(float)\n",
        "    tn = ((1 - y_true) * (1 - y_pred)).sum().astype(float)\n",
        "    fp = ((1 - y_true) * y_pred).sum().astype(float)\n",
        "    fn = (y_true * (1 - y_pred)).sum().astype(float)\n",
        "    \n",
        "    epsilon = 1e-7\n",
        "    \n",
        "    precision = tp / (tp + fp + epsilon)\n",
        "    recall = tp / (tp + fn + epsilon)\n",
        "    \n",
        "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
        "    #f1.requires_grad = is_training\n",
        "    return f1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVb3_x4Odmc-"
      },
      "source": [
        "# Training APIs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlBMyJ8qdmc-"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "\n",
        "\n",
        "def prepare_dataloader(df, trn_idx, val_idx, data_root='./input/cassava-leaf-disease-classification/train_images/'):\n",
        "    \n",
        "    from catalyst.data.sampler import BalanceClassSampler\n",
        "    \n",
        "    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n",
        "    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n",
        "        \n",
        "    train_ds = CassavaDataset(train_, data_root, transforms=get_train_transforms(), output_label=True, one_hot_label=False, do_fmix=False, do_cutmix=False)\n",
        "    valid_ds = CassavaDataset(valid_, data_root, transforms=get_valid_transforms(), output_label=True)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=CFG['train_bs'],\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        shuffle=True,        \n",
        "        num_workers=CFG['num_workers'],\n",
        "        #sampler=BalanceClassSampler(labels=train_['label'].values, mode=\"downsampling\")\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        valid_ds, \n",
        "        batch_size=CFG['valid_bs'],\n",
        "        num_workers=CFG['num_workers'],\n",
        "        shuffle=False,\n",
        "        pin_memory=False,\n",
        "    )\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def train_one_epoch(fold, epoch, model, loss_fn, optimizer, train_loader, device, scheduler=None, schd_batch_update=False):\n",
        "    model.train()\n",
        "\n",
        "    t = time.time()\n",
        "    running_loss = None\n",
        "\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for step, (imgs, image_labels) in pbar:\n",
        "        imgs = imgs.to(device).float()\n",
        "        image_labels = image_labels.to(device).long()\n",
        "\n",
        "        #print(image_labels.shape, exam_label.shape)\n",
        "        with autocast():\n",
        "       \n",
        "            image_preds = model(imgs).logits   #output = model(input)\n",
        "            #print(image_preds.shape, exam_pred.shape)\n",
        "\n",
        "            loss = loss_fn(image_preds, image_labels)\n",
        "            \n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if running_loss is None:\n",
        "                running_loss = loss.item()\n",
        "            else:\n",
        "                running_loss = running_loss * .99 + loss.item() * .01\n",
        "\n",
        "            if ((step + 1) %  CFG['accum_iter'] == 0) or ((step + 1) == len(train_loader)):\n",
        "                # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n",
        "\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad() \n",
        "                \n",
        "                if scheduler is not None and schd_batch_update:\n",
        "                    scheduler.step()\n",
        "\n",
        "            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n",
        "                description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
        "                \n",
        "                pbar.set_description(description)\n",
        "              \n",
        "    if scheduler is not None and not schd_batch_update:\n",
        "        scheduler.step()\n",
        "        \n",
        "def valid_one_epoch(isTrain, fold, epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False):\n",
        "    model.eval()\n",
        "\n",
        "    t = time.time()\n",
        "    loss_sum = 0\n",
        "    sample_num = 0\n",
        "    image_preds_all = []\n",
        "    image_targets_all = []\n",
        "    \n",
        "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
        "    for step, (imgs, image_labels) in pbar:\n",
        "        imgs = imgs.to(device).float()\n",
        "        image_labels = image_labels.to(device).long()\n",
        "        \n",
        "        image_preds = model(imgs).logits   #output = model(input)\n",
        "        #print(image_preds.shape, exam_pred.shape)\n",
        "        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n",
        "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
        "        \n",
        "        loss = loss_fn(image_preds, image_labels)\n",
        "        \n",
        "        loss_sum += loss.item()*image_labels.shape[0]\n",
        "        sample_num += image_labels.shape[0]  \n",
        "\n",
        "        if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n",
        "            description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n",
        "            pbar.set_description(description)\n",
        "    \n",
        "    image_preds_all = np.concatenate(image_preds_all)\n",
        "    image_targets_all = np.concatenate(image_targets_all)\n",
        "    print('validation multi-class accuracy = {:.4f}'.format((image_preds_all==image_targets_all).mean()))\n",
        "    print (\"Classification report: \", (classification_report(image_targets_all, image_preds_all)))\n",
        "    print (\"F1 micro averaging:\",(f1_score(image_targets_all, image_preds_all, average='micro')))\n",
        "\n",
        "\n",
        "    if isTrain == 1:\n",
        "       writer.add_scalar(\"Training loss\", loss_sum/sample_num, epoch + fold*33)\n",
        "       writer.add_scalar(\"Training accuracy\",(image_preds_all==image_targets_all).mean(), epoch + fold*33)\n",
        "    if isTrain == 0:\n",
        "       writer.add_scalar(\"Validation loss\", loss_sum/sample_num, epoch + fold*33)\n",
        "       writer.add_scalar(\"Validation accuracy\",(image_preds_all==image_targets_all).mean(), epoch + fold*33)\n",
        "\n",
        "    \n",
        "    if scheduler is not None:\n",
        "        if schd_loss_update:\n",
        "            scheduler.step(loss_sum/sample_num)\n",
        "        else:\n",
        "            scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqnwLV_Odmc_"
      },
      "source": [
        "# reference: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/173733\n",
        "class MyCrossEntropyLoss(_WeightedLoss):\n",
        "    def __init__(self, weight=None, reduction='mean'):\n",
        "        super().__init__(weight=weight, reduction=reduction)\n",
        "        self.weight = weight\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        lsm = F.log_softmax(inputs, -1)\n",
        "\n",
        "        if self.weight is not None:\n",
        "            lsm = lsm * self.weight.unsqueeze(0)\n",
        "\n",
        "        loss = -(targets * lsm).sum(-1)\n",
        "\n",
        "        if  self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "        elif  self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ickdxsj_ka4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "outputId": "89c2f211-bf2b-45f0-ca5d-2f24a58ba535"
      },
      "source": [
        "!pip3 install catalyst"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catalyst\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/61/cf38c5809599247fdc0f5b0d08f72873b7facd6367f2fd9bf714a82260ac/catalyst-21.6-py2.py3-none-any.whl (536kB)\n",
            "\r\u001b[K     |▋                               | 10kB 26.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 33.7MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 33.8MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 20.3MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 15.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 13.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 71kB 14.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 92kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 112kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 122kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 133kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 143kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 153kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 163kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 174kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 184kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 194kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 204kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 215kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 225kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 235kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 245kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 256kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 266kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 276kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 286kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 296kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 307kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 317kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 327kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 337kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 348kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 358kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 368kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 378kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 389kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 399kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 409kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 419kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 430kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 440kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 450kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 460kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 471kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 481kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 491kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 501kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 512kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 522kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 532kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 14.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.7/dist-packages (from catalyst) (4.41.1)\n",
            "Collecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 29.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from catalyst) (1.19.5)\n",
            "Collecting tensorboardX<2.3.0>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 53.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from catalyst) (1.9.0+cu102)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX<2.3.0>=2.1.0->catalyst) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->catalyst) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX<2.3.0>=2.1.0->catalyst) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX<2.3.0>=2.1.0->catalyst) (57.0.0)\n",
            "Installing collected packages: PyYAML, tensorboardX, catalyst\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 catalyst-21.6 tensorboardX-2.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVnVIEUcdmc_"
      },
      "source": [
        "# Main Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QNrQ6Didmc_"
      },
      "source": [
        "import torch.quantization\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "     # for training only, need nightly build pytorch\n",
        "\n",
        "    seed_everything(CFG['seed'])\n",
        "    \n",
        "    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train.shape[0]), train.label.values)\n",
        "    device = torch.device(CFG['device'])\n",
        "    print(device)\n",
        "        \n",
        "    model = CassvaImgClassifier(CFG['model_arch'], train.label.nunique(), pretrained=True).to(device)\n",
        "    #print(model)\n",
        "    model.load_state_dict(torch.load('./result/originalhuggingface/vit_base_patch16_224_fold_3_32'))\n",
        "    #model.eval()\n",
        "    \n",
        "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
        "        # we'll train fold 0 first\n",
        "        if fold < 4 or fold > 4:\n",
        "          continue \n",
        "\n",
        "        print('Training with {} started'.format(fold))\n",
        "\n",
        "        print(len(trn_idx), len(val_idx))\n",
        "        train_loader, val_loader = prepare_dataloader(train, trn_idx, val_idx, data_root='./input/cassava-leaf-disease-classification/train_images/')\n",
        "\n",
        "        \n",
        "        #model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "        #print(\"DONE QUANTIZATION!...................\")\n",
        "        scaler = GradScaler()   \n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n",
        "        #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=CFG['epochs']-1)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n",
        "        #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=25, \n",
        "        #                                                max_lr=CFG['lr'], epochs=CFG['epochs'], steps_per_epoch=len(train_loader))\n",
        "        \n",
        "        loss_tr = nn.CrossEntropyLoss().to(device) #MyCrossEntropyLoss().to(device)\n",
        "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "        \n",
        "        \n",
        "        for epoch in range(CFG['epochs']):\n",
        "          #if epoch > 27:\n",
        "            train_one_epoch(fold, epoch, model, loss_tr, optimizer, train_loader, device, scheduler=scheduler, schd_batch_update=False)\n",
        "          \n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                valid_one_epoch(1, fold, epoch, model, loss_fn, train_loader, device, scheduler=None, schd_loss_update=False)\n",
        "                valid_one_epoch(0, fold, epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False)\n",
        "               \n",
        "            torch.save(model.state_dict(),'./result/originalhuggingface/{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch))      \n",
        "            print(\"saving model...\")\n",
        "            #torch.save(model.state_dict(),'{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch))\n",
        "            \n",
        "        #torch.save(model.cnn_model.state_dict(),'{}/cnn_model_fold_{}_{}'.format(CFG['model_path'], fold, CFG['tag']))\n",
        "         #del model, optimizer, train_loader, val_loader, scaler, scheduler\n",
        "         #torch.cuda.empty_cache()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMwLOU_tdmdA"
      },
      "source": [
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtfoD_9dAUN5"
      },
      "source": [
        "#pip install tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SJHyzI-1gBT"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
