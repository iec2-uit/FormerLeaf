{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "papermill": {
      "duration": 8637.721674,
      "end_time": "2020-11-23T15:56:40.428882",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-11-23T13:32:42.707208",
      "version": "2.1.0"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYnmZ0SylA1O",
        "outputId": "f46c8616-ea27-4d0e-fa31-8806881a3801"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug 30 02:56:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCRoQMin1oGs"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hJT3EK2mIAi"
      },
      "source": [
        "%cd drive/MyDrive/Colab\\ Notebooks\n",
        "!pwd\n",
        "%cd ViT_Leaf_Desease/\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWkc1BUGdmcw"
      },
      "source": [
        "package_paths = [\n",
        "    './input/pytorch-image-models/pytorch-image-models-master', #'../input/efficientnet-pytorch-07/efficientnet_pytorch-0.7.0'\n",
        "    './input/FMix'\n",
        "]\n",
        "import sys; \n",
        "\n",
        "for pth in package_paths:\n",
        "    sys.path.append(pth)\n",
        "print(sys.path)    \n",
        "#from fmix import sample_mask, make_low_freq_image, binarise_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBNIAxAD50Sx"
      },
      "source": [
        "!pip3 install timm\n",
        "!pip3 install pydicom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9YIO3H_dmc2"
      },
      "source": [
        "from glob import glob\n",
        "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
        "import cv2\n",
        "from skimage import io\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "import cv2\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.nn.modules.loss import _WeightedLoss\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import timm\n",
        "\n",
        "import sklearn\n",
        "import warnings\n",
        "import joblib\n",
        "from sklearn.metrics import roc_auc_score, log_loss\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "import cv2\n",
        "import pydicom\n",
        "#from efficientnet_pytorch import EfficientNet\n",
        "from scipy.ndimage.interpolation import zoom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02VslVHzI7IV"
      },
      "source": [
        "CFG = {\n",
        "    'fold_num': 5,\n",
        "    'seed': 719,\n",
        "    #'model_arch': 'tf_efficientnet_b4_ns',\n",
        "    'model_arch': 'vit_base_patch16_224',\n",
        "    #'model_arch': 'vit_small_resnet50d_s3_224',\n",
        "    #'img_size': 512,\n",
        "    'img_size': 224,\n",
        "    'epochs': 33,\n",
        "    'train_bs': 32,\n",
        "    'valid_bs': 32,\n",
        "    'T_0': 10,\n",
        "    'lr': 1e-4,\n",
        "    'min_lr': 1e-6,\n",
        "    'weight_decay':1e-6,\n",
        "    'num_workers': 4,\n",
        "    'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
        "    'verbose_step': 1,\n",
        "    'device': 'cuda:0'\n",
        "  #  'device': 'cpu'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZowuKAsdmc4"
      },
      "source": [
        "#train = pd.read_csv('./input/cassava-leaf-disease-classification/train_short40.csv')\n",
        "train = pd.read_csv('./input/cassava-leaf-disease-classification/train.csv')\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J0eXl9Idmc5"
      },
      "source": [
        "train.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFZ6nnucdmc5"
      },
      "source": [
        "> We could do stratified validation split in each fold to make each fold's train and validation set looks like the whole train set in target distributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv1pWfSbdmc6"
      },
      "source": [
        "submission = pd.read_csv('./input/cassava-leaf-disease-classification/sample_submission.csv')\n",
        "submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C27ElN51dmc6"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUID6b6ndmc6"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "def get_img(path):\n",
        "    im_bgr = cv2.imread(path)\n",
        "    im_rgb = im_bgr[:, :, ::-1]\n",
        "    #print(im_rgb)\n",
        "    return im_rgb\n",
        "\n",
        "img = get_img('./input/cassava-leaf-disease-classification/train_images/1000015157.jpg')\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBhBzURVdmc7"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55GsDyVIdmc7"
      },
      "source": [
        "def rand_bbox(size, lam):\n",
        "    W = size[0]\n",
        "    H = size[1]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = np.int(W * cut_rat)\n",
        "    cut_h = np.int(H * cut_rat)\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "\n",
        "class CassavaDataset(Dataset):\n",
        "    def __init__(self, df, data_root, \n",
        "                 transforms=None, \n",
        "                 output_label=True, \n",
        "                 one_hot_label=False,\n",
        "                 do_fmix=False, \n",
        "                 fmix_params={\n",
        "                     'alpha': 1., \n",
        "                     'decay_power': 3., \n",
        "                     'shape': (CFG['img_size'], CFG['img_size']),\n",
        "                     'max_soft': True, \n",
        "                     'reformulate': False\n",
        "                 },\n",
        "                 do_cutmix=False,\n",
        "                 cutmix_params={\n",
        "                     'alpha': 1,\n",
        "                 }\n",
        "                ):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.df = df.reset_index(drop=True).copy()\n",
        "        self.transforms = transforms\n",
        "        self.data_root = data_root\n",
        "        self.do_fmix = do_fmix\n",
        "        self.fmix_params = fmix_params\n",
        "        self.do_cutmix = do_cutmix\n",
        "        self.cutmix_params = cutmix_params\n",
        "        \n",
        "        self.output_label = output_label\n",
        "        self.one_hot_label = one_hot_label\n",
        "        \n",
        "        if output_label == True:\n",
        "            self.labels = self.df['label'].values\n",
        "            #print(self.labels)\n",
        "            \n",
        "            if one_hot_label is True:\n",
        "                self.labels = np.eye(self.df['label'].max()+1)[self.labels]\n",
        "                #print(self.labels)\n",
        "            \n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index: int):\n",
        "        \n",
        "        # get labels\n",
        "        if self.output_label:\n",
        "            target = self.labels[index]\n",
        "          \n",
        "        img  = get_img(\"{}/{}\".format(self.data_root, self.df.loc[index]['image_id']))\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(image=img)['image']\n",
        "        \n",
        "        if self.do_fmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n",
        "            with torch.no_grad():\n",
        "                #lam, mask = sample_mask(**self.fmix_params)\n",
        "                \n",
        "                lam = np.clip(np.random.beta(self.fmix_params['alpha'], self.fmix_params['alpha']),0.6,0.7)\n",
        "                \n",
        "                # Make mask, get mean / std\n",
        "                mask = make_low_freq_image(self.fmix_params['decay_power'], self.fmix_params['shape'])\n",
        "                mask = binarise_mask(mask, lam, self.fmix_params['shape'], self.fmix_params['max_soft'])\n",
        "    \n",
        "                fmix_ix = np.random.choice(self.df.index, size=1)[0]\n",
        "                fmix_img  = get_img(\"{}/{}\".format(self.data_root, self.df.iloc[fmix_ix]['image_id']))\n",
        "\n",
        "                if self.transforms:\n",
        "                    fmix_img = self.transforms(image=fmix_img)['image']\n",
        "\n",
        "                mask_torch = torch.from_numpy(mask)\n",
        "                \n",
        "                # mix image\n",
        "                img = mask_torch*img+(1.-mask_torch)*fmix_img\n",
        "\n",
        "                #print(mask.shape)\n",
        "\n",
        "                #assert self.output_label==True and self.one_hot_label==True\n",
        "\n",
        "                # mix target\n",
        "                rate = mask.sum()/CFG['img_size']/CFG['img_size']\n",
        "                target = rate*target + (1.-rate)*self.labels[fmix_ix]\n",
        "                #print(target, mask, img)\n",
        "                #assert False\n",
        "        \n",
        "        if self.do_cutmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n",
        "            #print(img.sum(), img.shape)\n",
        "            with torch.no_grad():\n",
        "                cmix_ix = np.random.choice(self.df.index, size=1)[0]\n",
        "                cmix_img  = get_img(\"{}/{}\".format(self.data_root, self.df.iloc[cmix_ix]['image_id']))\n",
        "                if self.transforms:\n",
        "                    cmix_img = self.transforms(image=cmix_img)['image']\n",
        "                    \n",
        "                lam = np.clip(np.random.beta(self.cutmix_params['alpha'], self.cutmix_params['alpha']),0.3,0.4)\n",
        "                bbx1, bby1, bbx2, bby2 = rand_bbox((CFG['img_size'], CFG['img_size']), lam)\n",
        "\n",
        "                img[:, bbx1:bbx2, bby1:bby2] = cmix_img[:, bbx1:bbx2, bby1:bby2]\n",
        "\n",
        "                rate = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (CFG['img_size'] * CFG['img_size']))\n",
        "                target = rate*target + (1.-rate)*self.labels[cmix_ix]\n",
        "                \n",
        "            #print('-', img.sum())\n",
        "            #print(target)\n",
        "            #assert False\n",
        "                            \n",
        "        # do label smoothing\n",
        "        #print(type(img), type(target))\n",
        "        if self.output_label == True:\n",
        "            return img, target\n",
        "        else:\n",
        "            return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "545Xu8qG8NAx"
      },
      "source": [
        "#.!pip3 install --upgrade albumentations\n",
        "!pip3 install git+https://github.com/albumentations-team/albumentations.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT1kO4dCdmc8"
      },
      "source": [
        "# Define Train\\Validation Image Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBuv20Ipdmc8"
      },
      "source": [
        "from albumentations import (\n",
        "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
        "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
        "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
        "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
        ")\n",
        "\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "def get_train_transforms():\n",
        "    return Compose([\n",
        "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
        "            Transpose(p=0.5),\n",
        "            HorizontalFlip(p=0.5),\n",
        "            VerticalFlip(p=0.5),\n",
        "            ShiftScaleRotate(p=0.5),\n",
        "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
        "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
        "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
        "            CoarseDropout(p=0.5),\n",
        "            Cutout(p=0.5),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ], p=1.)\n",
        "  \n",
        "        \n",
        "def get_valid_transforms():\n",
        "    return Compose([\n",
        "            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n",
        "            Resize(CFG['img_size'], CFG['img_size']),\n",
        "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ], p=1.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82DHgqG90c7U"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWgR50Pvdmc9"
      },
      "source": [
        "# ViTSelfOutput"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKpqhWZYdmc9"
      },
      "source": [
        "from transformers import ViTModel, ViTFeatureExtractor, ViTForImageClassification\n",
        "from transformers import activations\n",
        "import math\n",
        "from transformers import modeling_outputs \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xG3yMrQDGQ1E"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUOWjh7RDAUh"
      },
      "source": [
        "class ViTIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = activations.ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "\n",
        "        return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irM-NmT5xV9Q"
      },
      "source": [
        "#ViTSelfAttention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FlOuUyexhKn"
      },
      "source": [
        "!pip install deepspeed\n",
        "from torch import nn\n",
        "from deepspeed.ops.sparse_attention import SparseSelfAttention, FixedSparsityConfig\n",
        "class MyViTSelfAttention(nn.Module):\n",
        "    def __init__(self, config,sparsity_config=FixedSparsityConfig(num_heads=12)  ):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n",
        "                f\"heads {config.num_attention_heads}.\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "      \n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "        #sparse attention configure\n",
        "        self.sparse_self_attention = SparseSelfAttention(sparsity_config)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        #print(key_layer.shape)\n",
        "        \n",
        "       \n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "     \n",
        "        #print(attention_probs.shape)\n",
        "        '''x = attention_probs\n",
        "        s = torch.sum(x, 3)\n",
        "        s1 = torch.sum(s, 2)\n",
        "        s2 = torch.sum(s1, 0)\n",
        "        sorted, indices = torch.sort(s2)\n",
        "        print(indices)'''\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        #print(attention_probs.shape)\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "        #print(head_mask)\n",
        "        \n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        '''x = context_layer\n",
        "        s = torch.sum(x, 3)\n",
        "        s1 = torch.sum(s, 2)\n",
        "        s2 = torch.sum(s1, 0)\n",
        "        sorted, indices = torch.sort(s2)\n",
        "        print(indices)'''\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        \n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "       \n",
        "\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s_oN3kGxykq"
      },
      "source": [
        "#ViTAttention\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oB8eVCbx1Dv"
      },
      "source": [
        "from transformers import modeling_utils\n",
        "class ViTSelfOutput(nn.Module):\n",
        "    \"\"\"\n",
        "    The residual connection is defined in ViTLayer instead of here (as is the case with other models), due to the\n",
        "    layernorm applied before each block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "class MyViTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = MyViTSelfAttention(config)\n",
        "        self.output = ViTSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = modeling_utils.find_pruneable_heads_and_indices(\n",
        "            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.attention.query = modeling_utils.prune_linear_layer(self.attention.query, index)\n",
        "        self.attention.key = modeling_utils.prune_linear_layer(self.attention.key, index)\n",
        "        self.attention.value = modeling_utils.prune_linear_layer(self.attention.value, index)\n",
        "        self.output.dense = modeling_utils.prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n",
        "        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
        "        self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n",
        "\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "     \n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XrY9ShLx85_"
      },
      "source": [
        "#ViTLayer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqr5BUKYx-8f"
      },
      "source": [
        "\n",
        "class ViTOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        hidden_states = hidden_states + input_tensor\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "class MyViTLayer(nn.Module):\n",
        "    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = MyViTAttention(config)\n",
        "        self.intermediate = ViTIntermediate(config)\n",
        "        self.output = ViTOutput(config)\n",
        "        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
        "        self_attention_outputs = self.attention(\n",
        "            self.layernorm_before(hidden_states),  # in ViT, layernorm is applied before self-attention\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "        \n",
        "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "        \n",
        "        # first residual connection\n",
        "        hidden_states = attention_output + hidden_states\n",
        "\n",
        "        # in ViT, layernorm is also applied after self-attention\n",
        "        layer_output = self.layernorm_after(hidden_states)\n",
        "\n",
        "        # TODO feedforward chunking not working for now\n",
        "        # layer_output = apply_chunking_to_forward(\n",
        "        #     self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, layer_output\n",
        "        # )\n",
        "\n",
        "        layer_output = self.intermediate(layer_output)\n",
        "\n",
        "        # second residual connection is done here\n",
        "        layer_output = self.output(layer_output, hidden_states)\n",
        "\n",
        "        outputs = (layer_output,) + outputs\n",
        "     \n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output)\n",
        "        return layer_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlimkAepyFqD"
      },
      "source": [
        "#ViTEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1NwCtx1yGp3"
      },
      "source": [
        "class MyViTEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([MyViTLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        head_mask=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    layer_head_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
        "        return modeling_outputs.BaseModelOutput(\n",
        "            last_hidden_state=hidden_states,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGwOk0pJ68Is"
      },
      "source": [
        "#ViTEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZSRPP9w69m5"
      },
      "source": [
        "import collections.abc\n",
        "\n",
        "def to_2tuple(x):\n",
        "    if isinstance(x, collections.abc.Iterable):\n",
        "        return x\n",
        "    return (x, x)\n",
        "\n",
        "class PatchEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Image to Patch Embedding.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_size=224, patch_size=16, num_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        image_size = to_2tuple(image_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.projection = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        \n",
        "    def forward(self, pixel_values):\n",
        "        batch_size, num_channels, height, width = pixel_values.shape\n",
        "      #  print(\"pixel_values.shape\")\n",
        "       # print(pixel_values.shape)\n",
        "        # FIXME look at relaxing size constraints\n",
        "        if height != self.image_size[0] or width != self.image_size[1]:\n",
        "            raise ValueError(\n",
        "                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]}).\"\n",
        "            )\n",
        "        x = self.projection(pixel_values).flatten(2).transpose(1, 2)      \n",
        "       # print(\"projection\")\n",
        "       # print(x.shape)  \n",
        "        return x\n",
        "\n",
        "class ViTEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Construct the CLS token, position and patch embeddings.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "        self.patch_embeddings = PatchEmbeddings(\n",
        "            image_size=config.image_size,\n",
        "            patch_size=config.patch_size,\n",
        "            num_channels=config.num_channels,\n",
        "            embed_dim=config.hidden_size,\n",
        "        )\n",
        "        num_patches = self.patch_embeddings.num_patches\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        batch_size = pixel_values.shape[0]\n",
        "        embeddings = self.patch_embeddings(pixel_values)\n",
        "        \n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        \n",
        "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
        "        embeddings = embeddings + self.position_embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWRZcYOcyNdo"
      },
      "source": [
        "#ViT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugzDJPLSyP2N"
      },
      "source": [
        "from transformers import ViTPreTrainedModel\n",
        "class MyViTModel(ViTPreTrainedModel):\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = ViTEmbeddings(config)\n",
        "        \n",
        "        self.encoder = MyViTEncoder(config)\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.pooler = ViTPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.patch_embeddings\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "        class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values=None,\n",
        "        head_mask=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Returns:\n",
        "\n",
        "        Examples::\n",
        "\n",
        "            >>> from transformers import ViTFeatureExtractor, ViTModel\n",
        "            >>> from PIL import Image\n",
        "            >>> import requests\n",
        "\n",
        "            >>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "            >>> image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "            >>> feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "            >>> model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "            >>> inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "            >>> outputs = model(**inputs)\n",
        "            >>> last_hidden_states = outputs.last_hidden_state\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if pixel_values is None:\n",
        "            raise ValueError(\"You have to specify pixel_values\")\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(pixel_values)\n",
        "\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "       \n",
        "        sequence_output = self.layernorm(sequence_output)\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return modeling_outputs.BaseModelOutputWithPooling(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpTeJGTLycLE"
      },
      "source": [
        "#ViTImageClassification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4Oei4MYyeRA"
      },
      "source": [
        "class MyViTForImageClassification(ViTPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.num_labels = config.num_labels\n",
        "        self.vit = MyViTModel(config, add_pooling_layer=False)\n",
        "\n",
        "        # Classifier head\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values=None,\n",
        "        head_mask=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the image classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Examples::\n",
        "\n",
        "            >>> from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "            >>> from PIL import Image\n",
        "            >>> import requests\n",
        "\n",
        "            >>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "            >>> image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "            >>> feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "            >>> model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "            >>> inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "            >>> outputs = model(**inputs)\n",
        "            >>> logits = outputs.logits\n",
        "            >>> # model predicts one of the 1000 ImageNet classes\n",
        "            >>> predicted_class_idx = logits.argmax(-1).item()\n",
        "            >>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.vit(\n",
        "            pixel_values,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        logits = self.classifier(sequence_output[:, 0, :])\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "        \n",
        "        return modeling_outputs.SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogN84GJI5q29"
      },
      "source": [
        "#CassavaModel\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UEYe0Z85tBq"
      },
      "source": [
        "from transformers import ViTModel, ViTForImageClassification\n",
        "\n",
        "class CassvaImgClassifier(nn.Module):\n",
        "    def __init__(self, model_arch, n_class, pretrained=False):\n",
        "        super().__init__()\n",
        "        self.model = MyViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "        #efficientNet initilization\n",
        "       # n_features = self.model.classifier.in_features\n",
        "        #self.model.classifier = nn.Linear(n_features, n_class)\n",
        "\n",
        "        #ViT initilization\n",
        "        self.model.classifier = nn.Linear(self.model.classifier.in_features, n_class)\n",
        "       \n",
        "       \n",
        "        '''\n",
        "        self.model.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            #nn.Linear(n_features, hidden_size,bias=True), nn.ELU(),\n",
        "            nn.Linear(n_features, n_class, bias=True)\n",
        "        )\n",
        "        '''\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a41XljavTfpN"
      },
      "source": [
        "def f1_loss(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:\n",
        "    '''Calculate F1 score. Can work with gpu tensors\n",
        "    \n",
        "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        `ndim` == 1. 0 <= val <= 1\n",
        "    \n",
        "    Reference\n",
        "    ---------\n",
        "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
        "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
        "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
        "    \n",
        "    '''\n",
        "    assert y_true.ndim == 1\n",
        "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
        "    \n",
        "    if y_pred.ndim == 2:\n",
        "        y_pred = y_pred.argmax(dim=1)\n",
        "    \n",
        "    print(y_true)\n",
        "    print(y_pred)\n",
        "    tp = (y_true * y_pred).sum().astype(float)\n",
        "    tn = ((1 - y_true) * (1 - y_pred)).sum().astype(float)\n",
        "    fp = ((1 - y_true) * y_pred).sum().astype(float)\n",
        "    fn = (y_true * (1 - y_pred)).sum().astype(float)\n",
        "    \n",
        "    epsilon = 1e-7\n",
        "    \n",
        "    precision = tp / (tp + fp + epsilon)\n",
        "    recall = tp / (tp + fn + epsilon)\n",
        "    \n",
        "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
        "    #f1.requires_grad = is_training\n",
        "    return f1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVb3_x4Odmc-"
      },
      "source": [
        "# Training APIs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlBMyJ8qdmc-"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "\n",
        "\n",
        "def prepare_dataloader(df, trn_idx, val_idx, data_root='./input/cassava-leaf-disease-classification/train_images/'):\n",
        "    \n",
        "    #from catalyst.data.sampler import BalanceClassSampler\n",
        "    \n",
        "    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n",
        "    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n",
        "        \n",
        "    train_ds = CassavaDataset(train_, data_root, transforms=get_train_transforms(), output_label=True, one_hot_label=False, do_fmix=False, do_cutmix=False)\n",
        "    valid_ds = CassavaDataset(valid_, data_root, transforms=get_valid_transforms(), output_label=True)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=CFG['train_bs'],\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        shuffle=True,        \n",
        "        num_workers=CFG['num_workers'],\n",
        "        #sampler=BalanceClassSampler(labels=train_['label'].values, mode=\"downsampling\")\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        valid_ds, \n",
        "        batch_size=CFG['valid_bs'],\n",
        "        num_workers=CFG['num_workers'],\n",
        "        shuffle=False,\n",
        "        pin_memory=False,\n",
        "    )\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def train_one_epoch(fold, epoch, model, loss_fn, optimizer, train_loader, device, scheduler=None, schd_batch_update=False):\n",
        "    model.train()\n",
        "\n",
        "    t = time.time()\n",
        "    running_loss = None\n",
        "\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    \n",
        "    for step, (imgs, image_labels) in pbar:\n",
        "        imgs = imgs.to(device).float()\n",
        "        image_labels = image_labels.to(device).long()\n",
        "        #print(image_labels)\n",
        "\n",
        "        #print(image_labels.shape, exam_label.shape)\n",
        "        with autocast():\n",
        "       \n",
        "            image_preds = model(imgs).logits   #output = model(input)\n",
        "            \n",
        "            #print(image_preds.shape, exam_pred.shape)\n",
        "\n",
        "            loss = loss_fn(image_preds, image_labels)\n",
        "            \n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if running_loss is None:\n",
        "                running_loss = loss.item()\n",
        "            else:\n",
        "                running_loss = running_loss * .99 + loss.item() * .01\n",
        "\n",
        "            if ((step + 1) %  CFG['accum_iter'] == 0) or ((step + 1) == len(train_loader)):\n",
        "                # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n",
        "\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad() \n",
        "                \n",
        "                if scheduler is not None and schd_batch_update:\n",
        "                    scheduler.step()\n",
        "\n",
        "            if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n",
        "                description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
        "                \n",
        "                pbar.set_description(description)\n",
        "              \n",
        "    if scheduler is not None and not schd_batch_update:\n",
        "        scheduler.step()\n",
        "        \n",
        "def valid_one_epoch(isTrain, fold, epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False):\n",
        "    model.eval()\n",
        "\n",
        "    t = time.time()\n",
        "    loss_sum = 0\n",
        "    sample_num = 0\n",
        "    image_preds_all = []\n",
        "    image_targets_all = []\n",
        "    \n",
        "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
        "    for step, (imgs, image_labels) in pbar:\n",
        "        imgs = imgs.to(device).float()\n",
        "        image_labels = image_labels.to(device).long()\n",
        "        \n",
        "        image_preds = model(imgs).logits   #output = model(input)\n",
        "        #print(model(imgs).attentions)\n",
        "        #print(image_preds.shape, exam_pred.shape)\n",
        "        image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n",
        "        image_targets_all += [image_labels.detach().cpu().numpy()]\n",
        "        \n",
        "        loss = loss_fn(image_preds, image_labels)\n",
        "        \n",
        "        loss_sum += loss.item()*image_labels.shape[0]\n",
        "        sample_num += image_labels.shape[0]  \n",
        "\n",
        "        if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n",
        "            description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n",
        "            pbar.set_description(description)\n",
        "    \n",
        "    image_preds_all = np.concatenate(image_preds_all)\n",
        "    image_targets_all = np.concatenate(image_targets_all)\n",
        "    print('validation multi-class accuracy = {:.4f}'.format((image_preds_all==image_targets_all).mean()))\n",
        "    print (\"Classification report: \", (classification_report(image_targets_all, image_preds_all)))\n",
        "    print (\"F1 micro averaging:\",(f1_score(image_targets_all, image_preds_all, average='micro')))\n",
        "\n",
        "\n",
        "    if isTrain == 1:\n",
        "       writer.add_scalar(\"Training loss\", loss_sum/sample_num, epoch + fold*33)\n",
        "       writer.add_scalar(\"Training accuracy\",(image_preds_all==image_targets_all).mean(), epoch + fold*33)\n",
        "    if isTrain == 0:\n",
        "       writer.add_scalar(\"Validation loss\", loss_sum/sample_num, epoch + fold*33)\n",
        "       writer.add_scalar(\"Validation accuracy\",(image_preds_all==image_targets_all).mean(), epoch + fold*33)\n",
        "\n",
        "    \n",
        "    if scheduler is not None:\n",
        "        if schd_loss_update:\n",
        "            scheduler.step(loss_sum/sample_num)\n",
        "        else:\n",
        "            scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqnwLV_Odmc_"
      },
      "source": [
        "# reference: https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/173733\n",
        "class MyCrossEntropyLoss(_WeightedLoss):\n",
        "    def __init__(self, weight=None, reduction='mean'):\n",
        "        super().__init__(weight=weight, reduction=reduction)\n",
        "        self.weight = weight\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        lsm = F.log_softmax(inputs, -1)\n",
        "\n",
        "        if self.weight is not None:\n",
        "            lsm = lsm * self.weight.unsqueeze(0)\n",
        "\n",
        "        loss = -(targets * lsm).sum(-1)\n",
        "\n",
        "        if  self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "        elif  self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVnVIEUcdmc_"
      },
      "source": [
        "# Main Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QNrQ6Didmc_"
      },
      "source": [
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "     # for training only, need nightly build pytorch\n",
        "\n",
        "    seed_everything(CFG['seed'])\n",
        "    \n",
        "    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train.shape[0]), train.label.values)\n",
        "    device = torch.device(CFG['device'])\n",
        "    print(device) \n",
        "  \n",
        "    model = torch.load('./result/prune5head/original_prune5head')\n",
        "    #model = torch.load('./result/prune3head/vit_base_patch16_224_fold_4_32')\n",
        "\n",
        "    print(model.model.vit.encoder.layer[0].attention.attention.num_attention_heads) \n",
        "    print(model.model.vit.encoder.layer[1].attention.attention.num_attention_heads) \n",
        "    print(model.model.vit.encoder.layer[2].attention.attention.num_attention_heads) \n",
        "    print(model.model.vit.encoder.layer[3].attention.attention.num_attention_heads) \n",
        "    print(model.model.vit.encoder.layer[4].attention.attention.num_attention_heads) \n",
        "    print(model.model.vit.encoder.layer[5].attention.attention.num_attention_heads) \n",
        "    print(model.model.vit.encoder.layer[6].attention.attention.num_attention_heads) \n",
        "    print(model.model.vit.encoder.layer[7].attention.attention.num_attention_heads) \n",
        "    print(model.model.vit.encoder.layer[8].attention.attention.num_attention_heads) \n",
        "    print(model.model.vit.encoder.layer[9].attention.attention.num_attention_heads) \n",
        "    print(model.model.vit.encoder.layer[10].attention.attention.num_attention_heads) \n",
        "    print(model.model.vit.encoder.layer[11].attention.attention.num_attention_heads) \n",
        "   \n",
        "\n",
        "    for fold, (trn_idx, val_idx) in enumerate(folds):        \n",
        "       # if fold < 4: \n",
        "       #   continue \n",
        "\n",
        "        print('Training with {} started'.format(fold))\n",
        "\n",
        "        print(len(trn_idx), len(val_idx))\n",
        "        train_loader, val_loader = prepare_dataloader(train, trn_idx, val_idx, data_root='./input/cassava-leaf-disease-classification/train_images/')\n",
        "\n",
        "        \n",
        "       \n",
        "        scaler = GradScaler()   \n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])        \n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n",
        "        \n",
        "        \n",
        "        loss_tr = nn.CrossEntropyLoss().to(device) \n",
        "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "        \n",
        "        \n",
        "        for epoch in range(CFG['epochs']):\n",
        "         # if epoch > 23:\n",
        "            train_one_epoch(fold, epoch, model, loss_tr, optimizer, train_loader, device, scheduler=scheduler, schd_batch_update=False)\n",
        "          \n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                valid_one_epoch(1, fold, epoch, model, loss_fn, train_loader, device, scheduler=None, schd_loss_update=False)\n",
        "                valid_one_epoch(0, fold, epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False)\n",
        "            \n",
        "            torch.save(model,'./result/prune5head/{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch))                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN4MCwY-fO2t"
      },
      "source": [
        "  !pip install tensorboard\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
